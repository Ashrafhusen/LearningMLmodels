{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGYiN6RKMzZhgUUVGK/vtV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashrafhusen/LearningMLmodels/blob/master/NLP_preProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji unidecode better_profanity langdetect symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy6NqCZ-EWgS",
        "outputId": "25c96e98-498b-4914-f87c-2dbeaf920506"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Collecting better_profanity\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=41997ac0bf50a041dc713f9d81ddd11fe0e7661cc8da1d916f571ab9788808a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, editdistpy, better_profanity, symspellpy\n",
            "Successfully installed better_profanity-0.7.0 editdistpy-0.1.6 langdetect-1.0.9 symspellpy-6.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6vdlyYyHEFwx"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "import emoji\n",
        "from unidecode import unidecode\n",
        "from better_profanity import profanity\n",
        "from langdetect import detect\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "ujMNML7WEgh8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profanity.load_censor_words()"
      ],
      "metadata": {
        "id": "1cpIN1PHHLfP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = \"frequency_dictionary_en_82_765.txt\""
      ],
      "metadata": {
        "id": "eI_XTk4LHSY2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(dictionary_path):\n",
        "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
      ],
      "metadata": {
        "id": "FZqaEBuKHXCJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_elongated_words(text):\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1', text)"
      ],
      "metadata": {
        "id": "-TaWeWS6HZ-9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = unidecode(text)\n",
        "  text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "  text = re.sub(r\"[^a-zA-Z\\s:]\", '', text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  text = reduce_elongated_words(text)\n",
        "\n",
        "  if sym_spell.words:\n",
        "        suggestions = sym_spell.lookup_compound(text, max_edit_distance=2)\n",
        "        text = suggestions[0].term if suggestions else text\n",
        "\n",
        "  text = profanity.censor(text)\n",
        "\n",
        "  doc = nlp(text)\n",
        "  clean_tokens = [\n",
        "      token.lemma_ for token in doc\n",
        "      if token.is_alpha and not token.is_stop and not token.ent_type_\n",
        "  ]\n",
        "\n",
        "  return \" \".join(clean_tokens)"
      ],
      "metadata": {
        "id": "R_cn_UsnFBgX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"I'm LOVING #NLP!!! It's soooo coooool 😍😍. Visit http://example.com. You idiot!\""
      ],
      "metadata": {
        "id": "W_LNhziDGJBv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = preprocess_text(raw_text)"
      ],
      "metadata": {
        "id": "t6d820eFGOdM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaned:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiPJ7CRmGP2E",
        "outputId": "bce5c3bd-f104-4875-8d4b-29a18dd60859"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned: m love nlp col visit idiot\n"
          ]
        }
      ]
    }
  ]
}